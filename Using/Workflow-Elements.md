A Bitnobi Workflow starts with one or more `Datasource` elements, performs an optional sequence of `Operations` and ends with a `Result` element. Here is a sample workflow:

[[images/workflow-example.png]]

### Configuring a Workflow Element
When you drag a workflow element onto the canvas and then click on it, the properties are displayed in the right-hand pane. Pressing on the `Configuration` section in the properties pane opens up the parameters necessary to configure the element. Each element type has a different set of configuration parameters.

### Apply Button
After you set the properties, clicking on the `Apply` button will populate a preview of a few rows of output generated by this workflow element and display it in the `Properties` pane at the bottom of the screen. The preview output is used to fetch the column names for the subsequent element in the workflow, so it is crucial to press `Apply` on each element while creating a workflow.

### Customizing Workflow Elements
The top of the properties pane for each element has a `Customize` section. When you click on it you are then able to modify the element label and background colour. You can also change the size of the element on the canvas by grabbing a corner and dragging it.

### Element Types
Below is a summary of the element types. Clicking on the element name will jump to a detailed description.

|Category|Element Name|Description|
|--------|------------|----|
|**Datasources**| | provides ways of getting data|
| |[Audit Log](#audit-log)| records timestamped Bitnobi events |
| |[Datasource](#datasource) |uses result set from the last run of the specified workflow. |
| |[Importer](#importer) | allows user to upload a .csv or .json file to use as a datasource |
| |[Jupyter](#jupyter) | uses output of a Jupyter Notebooks session as a datasource. |
| |[SQL](#sql)| allows specifying an external SQL database and query to use as a datasource|
| |[Workflow](#workflow) |runs the specified workflow and uses its result set |
| **Operations**|  | elements in a workflow that modify data|
| |[Alter](#alter)| allows renaming fields and changing field datatype| |
| |[Group](#group)| group rows that have the same values into summary rows and perform an aggregation on one of the columns.|
| |[Join](#join)| combines two data flows based on the common columns in the two flows (like a SQL inner join)|
| |[Python](#python)| allows the user to create a Python function to modify the row contents or create new rows. |
| |[Select](#select)| allows filtering out columns and/or filtering rows based on matching conditions (where clause)|
| |[Union](#union) |concatenates two data sets with the same schema (like SQL Union). |
|**Result**|| A valid workflow must have one and only one Result. |
| |[Result](#result)|Identifies the end of the workflow. |

Below are more details for each of the element types.
*** 
# Datasources

## Audit Log
* only accessible to an admin user
* records timestamped events and the user requesting it for:
  * user signin/signup/signout or modification,
  * policy, report and workflow creation, modification deletion
  * workflow start, end,
  * datasource access

## Datasource
* takes the result of a workflow execution as input to the current workflow
* the properties pane displays a drop-down list of workflow names with result sets available to you. 
Only those results where the a policy matches your user attributes will appear in this list.

## Importer
* allows uploading an external data file to Bitnobi to use a a datasource.
* supports comma separated values (.csv) files and JSON (.json) format files.
* all columns are set to a "string" datatype, so it may be necessary to use the `Alter` element to 
change the datatype for numeric columns, especially if you plan to use this column in the "Where Condition" clause of a `Select` element.


## SQL
* allows connecting to and querying an external MySQL database to use as a datasource

## Workflow
* executes the named workflow and uses its result set as a datasource.
* can define a "chain" of workflows where the output of one is used an an input to another. This chain will be executed sequentially.
* the properties pane displays a drop-down list of workflow names with result sets available to you. Only those results where the a policy matches your user attributes will appear in this list.
* while editing workflow, "Apply" for this element will not execute the underlying workflow but rather depends on previous workflow run to provide preview data and schema. 

*** 
# Operations
## Alter
* allows changing the names of columns in the data flow
* allows changing column datatypes. Some of the datasources may import numeric columns as a "string" datatype which can cause unexpected behaviour in a Select Where clause. 

## Group
* groups rows that have the same values into summary rows, like "find the number of customers in each country". An aggregate function (count, max, min, sum, avg) is used to group the result-set by one or more columns.
* in the properties pane, select the columns that you wish to group by, then an aggregation functions and the column to aggregate. 
* for example if you have a dataset listing customers along with a country column, then you can find the number of customers in each country if you GroupBy "Country" and select the aggregation function "count", the result will be a dataset with the columns "Country" and "count". 
* If you have a dataset listing customer transactions with columns for "timestamp", "product", "Customer name", "amount", you can find the average sale per customer by doing a GroupBy "Customer name", select aggregation function "avg" and field "amount".

## Join
* takes two data flow inputs and generates an output that is an "inner join" of the two data sets.
* in the properties pane, select the column from each data input that will be used as common columns for the join.
* can also be used to perform a "lookup" function. For example lets say that input 1 has a list of names as well as a "status-id" column containing either 0 or 1. And lets say that input 2 has two columns:
```
status-id,status-text
0, inactive
1, active
```
Then the join operation choosing "status-id" as the common column will produce an output like input 1 but with an extra column "status-text" containing "inactive" wherever "status-id" is 0 and "active" wherever "status-id" is 1.
* note that both inputs to a Join element must be on the same Bitnobi server. A Data Mover is required to move the data flow from an ExternalDS to the local server prior to a Join.
## Python
* allows writing a Python function to modify the data flow. 
* in the properties of this element, there is an 'Open Python Editor' button that pops up an editor where you can compose your Python code.
* pressing the "Sample Template" button will insert a sample program:
```
 # gets and displays preview input from canvas
bitnobiInput = io.getInput()
for row in bitnobiInput:
    print(row)

# reset input cursor, use only if needed to loop through the input another time
io.resetInput()

# write to output
io.writeOutput(['name','id'])
io.writeOutput(['justin','1'])
io.writeOutput(['john','2'])
```
* Bitnobi provides the built-in functions `io.getInput()` for accessing the input data stream one row at a time and `io.writeOutput()` to write out one row at a time.
* pressing the "Run" button in the upper right corner of the Python editor will execute the program with preview data and display the print statements in "Console" pane. The console output is for debugging only. You must press the "Run" button to generate preview output for the next element.
* Now if you press the "Display output options" button it will pop up a pane to display the output from `io.writeOutput()` statements. This is the output data that will be relayed to the next workflow element.
* pressing the "Save" button saves the code and exits the Python editor. Pressing "Apply" in the Python properties will populate the Preview pane. Note that the "Apply" button does not execute the Python code, it is important to press the "Run" button before existing the Python editor otherwise no output preview data will be generated.

## Select
* allows filtering out columns in the data stream.
* allows restricting which rows are passed along by creating "Where" conditions on specific columns (fields). Note that for numeric columns, it may be necessary to use the `Alter` block to cast the datatype as "Integer" or "Float" to get proper numeric comparisons (e.g. selecting rows where id < 50). For string datatype the comparison is in dictionary (lexicographic) order. String comparisons can also use the "like" operation which matches substrings.

## Union
* concatenates two data sets with the same schema (like SQL Union).
* It does not remove duplicate rows between the two input data sets.

*** 
# Result
## Result
* There must be exactly one `Result` element to indicate the end of the workflow.


